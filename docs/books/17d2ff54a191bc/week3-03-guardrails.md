---
title: "AIの安全装置：Guardrailsで品質と安全性を守る"
---

## なぜLLMに「ガードレール」が必要なのか？

<!--
この章で扱う内容：
- LLMアプリ特有のセキュリティリスク
- プロンプトインジェクションとは何か
- 有害出力・不適切な回答のリスク
- 出力フォーマットが崩れる問題
-->

## プロンプトインジェクションの脅威

<!--
この章で扱う内容：
- プロンプトインジェクションの具体例
- 「システムプロンプトを無視して〜」攻撃
- RAGを悪用した間接的なインジェクション
- TypeScriptエンジニア向け：SQLインジェクションとの類似性
-->

## 実践1：入力バリデーションの実装

<!--
この章で扱う内容：
- 悪意あるパターンの検知（キーワードベース）
- 入力長の制限
- FastAPIのリクエストバリデーションとの統合
- 実装例：server.pyへの組み込み
-->

## 実践2：構造化出力の強制（Output Parser）

<!--
この章で扱う内容：
- なぜ構造化出力が重要か（フロントエンドとの連携）
- PydanticとLangChain OutputParserの組み合わせ
- JSON形式での出力強制
- パースエラー時のリトライ戦略
- 実装例：ChatResponseの構造化
-->

## 実践3：出力フィルタリング

<!--
この章で扱う内容：
- 有害コンテンツの検知
- 機密情報の漏洩防止（APIキーなどのパターンマッチ）
- Langfuseでのフィルタリングログ記録
-->

## Guardrailsと評価の連携

<!--
この章で扱う内容：
- 前章の評価データセットにGuardrails関連のテストケースを追加
- プロンプトインジェクション耐性の測定
- 構造化出力の成功率追跡
-->

## まとめ

<!--
まとめのポイント：
1. LLMアプリには従来のWebアプリとは異なるセキュリティリスクがある
2. 入力バリデーション + 出力バリデーション の二重防御が基本
3. Guardrailsは「制限」ではなく「品質保証」である
-->

次章では、RAGの検索品質を改善するテクニックについて学びます。より正確な情報検索が、より正確な回答につながります。

## 本章のソースコード

本章で作成したコードは、以下のGitHubブランチで確認できます。

*   [feature/week3-03-guardrails](https://github.com/duotaro/my-ai-bot/tree/feature/week3-03-guardrails)
